# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLQHIf_EPkkJjICv4fXK3g87RdmGz0XW
"""

# -----------------------------
# Model Definitions
# -----------------------------
class SharedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, num_layers=1, dropout=DROPOUT):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,
                           dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]
        return self.dropout(out)

# STL Model
class STLModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, dropout=DROPOUT, num_classes=NUM_CLASSES):
        super().__init__()
        self.rsrp_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.rsrq_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.sinr_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.cls_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.rsrp_head = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.rsrq_head = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.sinr_head = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.cls_head = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, num_classes))

    def forward(self, x):
        h_rsrp = self.rsrp_lstm(x)
        h_rsrq = self.rsrq_lstm(x)
        h_sinr = self.sinr_lstm(x)
        h_cls = self.cls_lstm(x)
        rsrp_out = self.rsrp_head(h_rsrp)
        rsrq_out = self.rsrq_head(h_rsrq)
        sinr_out = self.sinr_head(h_sinr)
        cls_out = self.cls_head(h_cls)
        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

# Hard Sharing Model
class HardSharingModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, dropout=DROPOUT, num_classes=NUM_CLASSES):
        super().__init__()
        self.shared = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.rsrp_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden_size//2, 1))
        self.rsrq_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden_size//2, 1))
        self.sinr_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden_size//2, 1))
        self.cls_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden_size//2, num_classes))

    def forward(self, x):
        shared_feat = self.shared(x)
        rsrp_out = self.rsrp_tower(shared_feat)
        rsrq_out = self.rsrq_tower(shared_feat)
        sinr_out = self.sinr_tower(shared_feat)
        cls_out = self.cls_tower(shared_feat)
        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

# MMOE Model
class GatingNetwork(nn.Module):
    def __init__(self, input_size, num_experts, hidden_size=32, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_experts)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return F.softmax(self.fc2(x), dim=1)

class MMOEModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, num_experts=NUM_EXPERTS,
                 num_tasks=NUM_TASKS, dropout=DROPOUT, num_classes=NUM_CLASSES):
        super().__init__()
        self.num_experts = num_experts
        self.num_tasks = num_tasks

        self.shared_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            ) for _ in range(num_experts)
        ])

        self.gates = nn.ModuleList([
            GatingNetwork(hidden_size, num_experts, hidden_size=32, dropout=0.1)
            for _ in range(num_tasks)
        ])

        self.rsrp_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.rsrq_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.sinr_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, 1))
        self.cls_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Linear(hidden_size//2, num_classes))

    def forward(self, x):
        shared_features = self.shared_lstm(x)
        expert_outputs = torch.stack([e(shared_features) for e in self.experts], dim=1)

        task_inputs = []
        for g in self.gates:
            gw = g(shared_features).unsqueeze(-1)
            task_in = torch.sum(expert_outputs * gw, dim=1)
            task_inputs.append(task_in)

        rsrp_out = self.rsrp_tower(task_inputs[0])
        rsrq_out = self.rsrq_tower(task_inputs[1])
        sinr_out = self.sinr_tower(task_inputs[2])
        cls_out = self.cls_tower(task_inputs[3])

        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

# Cross-Stitch Model
class CrossStitchUnit(nn.Module):
    def __init__(self, num_tasks):
        super().__init__()
        init = torch.eye(num_tasks)
        self.cross = nn.Parameter(init)

    def forward(self, task_feats):
        stacked = torch.stack(task_feats, dim=0)
        combined = torch.einsum('ij,jbh->ibh', self.cross, stacked)
        return [combined[i] for i in range(combined.shape[0])]

class CrossStitchModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, dropout=DROPOUT, num_classes=NUM_CLASSES):
        super().__init__()
        self.rsrp_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.rsrq_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.sinr_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.cls_lstm = SharedLSTM(input_size, hidden_size, dropout=dropout)
        self.cs1 = CrossStitchUnit(num_tasks=NUM_TASKS)
        self.cs2 = CrossStitchUnit(num_tasks=NUM_TASKS)
        self.dropout = nn.Dropout(dropout)
        self.rsrp_head = nn.Linear(hidden_size, 1)
        self.rsrq_head = nn.Linear(hidden_size, 1)
        self.sinr_head = nn.Linear(hidden_size, 1)
        self.cls_head = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        f_rsrp = self.rsrp_lstm(x)
        f_rsrq = self.rsrq_lstm(x)
        f_sinr = self.sinr_lstm(x)
        f_cls = self.cls_lstm(x)
        task_feats = [f_rsrp, f_rsrq, f_sinr, f_cls]
        out1 = self.cs1(task_feats)
        out1 = [self.dropout(t) for t in out1]
        out2 = self.cs2(out1)
        rsrp_out = self.rsrp_head(out2[0])
        rsrq_out = self.rsrq_head(out2[1])
        sinr_out = self.sinr_head(out2[2])
        cls_out = self.cls_head(out2[3])
        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

# PLE Model
class PLELayer(nn.Module):
    def __init__(self, input_dim, num_shared_experts, num_task_experts, num_tasks, hidden_dim):
        super().__init__()
        self.num_tasks = num_tasks
        self.num_shared_experts = num_shared_experts
        self.num_task_experts = num_task_experts

        self.shared_experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(DROPOUT)
            ) for _ in range(num_shared_experts)
        ])

        self.task_experts = nn.ModuleList([
            nn.ModuleList([
                nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(DROPOUT)
                ) for _ in range(num_task_experts)
            ]) for _ in range(num_tasks)
        ])

        self.gates = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, 32),
                nn.ReLU(),
                nn.Linear(32, num_shared_experts + num_task_experts),
                nn.Softmax(dim=1)
            ) for _ in range(num_tasks)
        ])

        self.extraction_gate = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, num_shared_experts + num_tasks * num_task_experts),
            nn.Softmax(dim=1)
        )

    def forward(self, task_inputs):
        batch_size = task_inputs[0].shape[0]
        shared_expert_outputs = [expert(task_inputs[0]) for expert in self.shared_experts]
        task_outputs = []
        all_task_expert_outputs = []

        for task_id in range(self.num_tasks):
            task_expert_outputs = [expert(task_inputs[task_id])
                                   for expert in self.task_experts[task_id]]
            all_task_expert_outputs.extend(task_expert_outputs)
            all_expert_outputs = shared_expert_outputs + task_expert_outputs
            expert_stack = torch.stack(all_expert_outputs, dim=1)
            gate_weights = self.gates[task_id](task_inputs[task_id]).unsqueeze(-1)
            task_output = torch.sum(expert_stack * gate_weights, dim=1)
            task_outputs.append(task_output)

        all_experts_combined = shared_expert_outputs + all_task_expert_outputs
        all_experts_stack = torch.stack(all_experts_combined, dim=1)
        extraction_weights = self.extraction_gate(task_inputs[0]).unsqueeze(-1)
        shared_output = torch.sum(all_experts_stack * extraction_weights, dim=1)

        return task_outputs, shared_output

class PLEModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, num_shared_experts=3,
                 num_task_experts=2, num_layers=2, num_classes=NUM_CLASSES):
        super().__init__()
        self.shared_lstm = SharedLSTM(input_size, hidden_size, dropout=DROPOUT)
        self.num_tasks = NUM_TASKS
        self.num_layers = num_layers

        self.ple_layers = nn.ModuleList([
            PLELayer( # âœ… Changed to True for better training
                input_dim=hidden_size,
                num_shared_experts=num_shared_experts,
                num_task_experts=num_task_experts,
                num_tasks=self.num_tasks,
                hidden_dim=hidden_size
            ) for _ in range(num_layers)
        ])

        self.rsrp_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(),
                                       nn.Dropout(DROPOUT), nn.Linear(hidden_size//2, 1))
        self.rsrq_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(),
                                       nn.Dropout(DROPOUT), nn.Linear(hidden_size//2, 1))
        self.sinr_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(),
                                       nn.Dropout(DROPOUT), nn.Linear(hidden_size//2, 1))
        self.cls_tower = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU(),
                                      nn.Dropout(DROPOUT), nn.Linear(hits, dropout=dropout)
    elif name == "crossstitch":
        return CrossStitchModel(input_size, hidden_size, dropout)
    elif name == "ple":
        return PLEModel(input_size, hidden_size)
    elif name == "attention":
        return AttentionMTLModel(input_size, hidden_size, dropout)
    else:
        raise ValueError(f"Unknown model: {name}. Available: stl, hardsharing, mmoe, crossstitch, ple, attention")dden_size//2, num_classes))

    def forward(self, x):
        shared_feat = self.shared_lstm(x)
        task_feats = [shared_feat for _ in range(self.num_tasks)]

        for layer in self.ple_layers:
            task_feats, shared_feat = layer(task_feats)

        rsrp_out = self.rsrp_tower(task_feats[0])
        rsrq_out = self.rsrq_tower(task_feats[1])
        sinr_out = self.sinr_tower(task_feats[2])
        cls_out = self.cls_tower(task_feats[3])

        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

# Attention MTL Model
class MultiTaskAttention(nn.Module):
    def __init__(self, hidden_size=HIDDEN_SIZE, num_tasks=NUM_TASKS):
        super().__init__()
        self.num_tasks = num_tasks
        self.task_queries = nn.Parameter(torch.randn(num_tasks, hidden_size))
        self.key_proj = nn.Linear(hidden_size, hidden_size)
        self.value_proj = nn.Linear(hidden_size, hidden_size)

    def forward(self, shared_seq):
        batch, seq_len, hidden = shared_seq.shape
        keys = self.key_proj(shared_seq)
        values = self.value_proj(shared_seq)
        task_outputs = []
        for t in range(self.num_tasks):
            q = self.task_queries[t].unsqueeze(0).expand(batch, -1).unsqueeze(1)
            scores = torch.bmm(q, keys.transpose(1,2))
            attn = F.softmax(scores / (hidden ** 0.5), dim=-1)
            attended = torch.bmm(attn, values).squeeze(1)
            task_outputs.append(attended)
        return task_outputs

class AttentionMTLModel(nn.Module):
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, dropout=DROPOUT, num_classes=NUM_CLASSES):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0)
        self.attention = MultiTaskAttention(hidden_size=hidden_size, num_tasks=NUM_TASKS)
        self.rsrp_tower = nn.Linear(hidden_size, 1)
        self.rsrq_tower = nn.Linear(hidden_size, 1)
        self.sinr_tower = nn.Linear(hidden_size, 1)
        self.cls_tower = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        task_feats = self.attention(lstm_out)
        rsrp_out = self.rsrp_tower(task_feats[0])
        rsrq_out = self.rsrq_tower(task_feats[1])
        sinr_out = self.sinr_tower(task_feats[2])
        cls_out = self.cls_tower(task_feats[3])
        regression_out = torch.cat([rsrp_out, rsrq_out, sinr_out], dim=1)
        return regression_out, cls_out

def get_model(name, input_size, hidden_size, dropout, num_experts):
    """Factory function to create models"""
    name = name.lower()

    if name == "stl":
        return STLModel(input_size, hidden_size, dropout)
    elif name == "hardsharing":
        return HardSharingModel(input_size, hidden_size, dropout)
    elif name == "mmoe":
        return MMOEModel(input_size, hidden_size, num_experts=num_experts, dropout=dropout)
    elif name == "crossstitch":
        return CrossStitchModel(input_size, hidden_size, dropout)
    elif name == "ple":
        return PLEModel(input_size, hidden_size)
    elif name == "attention":
        return AttentionMTLModel(input_size, hidden_size, dropout)
    else:
        raise ValueError(f"Unknown model: {name}. Available: stl, hardsharing, mmoe, crossstitch, ple, attention")