# -*- coding: utf-8 -*-
"""Load and Preprocess Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLQHIf_EPkkJjICv4fXK3g87RdmGz0XW
"""

# Commented out IPython magic to ensure Python compatibility.
# -----------------------------
# Imports
# -----------------------------
import os
import random
import time
import glob
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, ConcatDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from thop import profile, clever_format

# %cd drive/MyDrive

# Set device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# Config
# -----------------------------
BASEPATH = "Results"
os.makedirs(BASEPATH, exist_ok=True)
os.makedirs(f"{BASEPATH}/loss_curves", exist_ok=True)
os.makedirs(f"{BASEPATH}/test_curves", exist_ok=True)
os.makedirs(f"{BASEPATH}/confusion_matrices", exist_ok=True)

SEED = 50
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
set_seed()

# Model hyperparameters
HIDDEN_SIZE = 256
DROPOUT = 0.3
NUM_TASKS = 4
NUM_CLASSES = 2
NUM_EXPERTS = 4  # For MMOE

# Training hyperparameters
BATCH_SIZE = 64
LR = 0.001
WD = 1e-5
EPOCHS = 300
N_STEPS = 35
FEATURES = ['RSRP', 'RSRQ', 'SINR']
REG_TARGETS = ['RSRP', 'RSRQ', 'SINR']
CLS_TARGET = 'lab_inf'



# Option 2: Train all models (one best model per architecture)
MODELS_TO_TRAIN = ['mmoe', 'hardsharing', 'stl', 'crossstitch', 'ple', 'attention']

# -----------------------------
# Utilities
# -----------------------------
def calculate_model_metrics_thop(model, input_size, seq_len, device):
    metrics = {}
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
    metrics['total_params'] = total_params
    metrics['trainable_params'] = trainable_params
    metrics['model_size_mb'] = model_size_mb

    try:
        dummy_input = torch.randn(1, seq_len, input_size).to(device)
        flops, params = profile(model, inputs=(dummy_input,), verbose=False)
        flops_readable, params_readable = clever_format([flops, params], "%.3f")
        metrics['flops'] = flops
        metrics['flops_readable'] = flops_readable
        metrics['params_readable'] = params_readable
    except Exception as e:
        print("thop error:", e)
        metrics['flops'] = None
        metrics['flops_readable'] = "N/A"
    return metrics

def calculate_inference_time(model, test_loader, device, num_runs=100):
    model.eval()
    inference_times = []
    sample_count = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if sample_count >= num_runs:
                break

            if isinstance(batch, (tuple, list)):
                data = batch[0]
            else:
                data = batch

            data = data.to(device)
            batch_size = data.size(0)

            start = time.perf_counter()
            _ = model(data)
            elapsed = time.perf_counter() - start

            per_sample_time = elapsed / batch_size
            inference_times.extend([per_sample_time] * batch_size)
            sample_count += batch_size

    if len(inference_times) == 0:
        return None

    return np.mean(inference_times)

def create_windows(df, n_steps):
    """Create sliding windows with properly aligned labels"""
    X_windows, y_reg_windows, y_cls_windows = [], [], []

    if len(df) < n_steps + 1:
        return np.array([]), np.array([]), np.array([])

    for i in range(len(df) - n_steps):
        # Input window: [i : i+n_steps]
        X_win = df[FEATURES].iloc[i:i+n_steps].values

        # Regression target: predict at time step i+n_steps
        y_reg = df[REG_TARGETS].iloc[i+n_steps].values

        # Classification target: predict at time step i+n_steps-1
        y_cls = df[CLS_TARGET].iloc[i+n_steps-1]

        X_windows.append(X_win)
        y_reg_windows.append(y_reg)
        y_cls_windows.append(y_cls)

    X = np.array(X_windows)
    y_reg = np.array(y_reg_windows)
    y_cls = np.array(y_cls_windows).reshape(-1, 1)

    return X, y_reg, y_cls

def plot_confusion_matrix(y_true, y_pred, dataset_name, basepath):
    """Plot and save confusion matrix"""
    # Flatten arrays if needed
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()

    unique_classes = np.unique(np.concatenate([y_true, y_pred]))
    n_classes = len(unique_classes)

    if n_classes < 2:
        print(f"Warning: Only {n_classes} class(es) found. Skipping confusion matrix.")
        return None

    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[f'Class {i}' for i in range(NUM_CLASSES)],
                yticklabels=[f'Class {i}' for i in range(NUM_CLASSES)])
    plt.title(f'Confusion Matrix - {dataset_name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')

    accuracy = np.sum(np.diag(cm)) / np.sum(cm) * 100 if np.sum(cm) > 0 else 0.0
    plt.text(0.5, -0.15, f'Accuracy: {accuracy:.2f}%',
             ha='center', transform=plt.gca().transAxes, fontsize=12)

    filename = f"{basepath}/confusion_matrices/cm_{dataset_name}.png"
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    plt.close()

    try:
        labels_present = np.unique(y_true)
        target_names_present = [f'Class {i}' for i in labels_present]

        report = classification_report(
            y_true, y_pred,
            labels=labels_present,
            target_names=target_names_present,
            zero_division=0
        )

        report_filename = f"{basepath}/confusion_matrices/report_{dataset_name}.txt"
        with open(report_filename, 'w') as f:
            f.write(f"Classification Report - {dataset_name}\n")
            f.write("="*60 + "\n")
            f.write(f"Classes present: {labels_present}\n")
            f.write(f"Total samples: {len(y_true)}\n")
            f.write("="*60 + "\n")
            f.write(report)
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(cm))
    except Exception as e:
        print(f"Warning: Could not generate classification report: {e}")

    return cm, accuracy

# -----------------------------
# Load and Preprocess Data
# -----------------------------
print("\n" + "="*80)
print("LOADING AND PREPROCESSING DATA (PER-CSV SCALING)")
print("="*80)

csv_files = glob.glob("FccIQ_Dataset/Lvl_*.csv")
if len(csv_files) == 0:
    raise FileNotFoundError("No CSV files found at FccIQ_Dataset/Lvl_*.csv")

print(f"Found {len(csv_files)} dataset files")

# Lists to collect all processed datasets
train_datasets = []
val_datasets = []
test_datasets = []

total_train_windows = 0
total_val_windows = 0
total_test_windows = 0

# Process each CSV independently
for file in csv_files:
    df = pd.read_csv(file)
    dataset_name = os.path.basename(file).replace(".csv", "")
    print(f"\n{'='*60}")
    print(f"Processing {dataset_name}")
    print(f"{'='*60}")
    print(f"Original shape: {df.shape}")

    # Clean data
    df.replace('-', np.nan, inplace=True)
    df.dropna(subset=FEATURES + REG_TARGETS + [CLS_TARGET], inplace=True)
    print(f"After cleaning: {df.shape}")

    # Split this dataset (80-10-10)
    n = len(df)
    train_end = int(0.8 * n)
    val_end = int(0.9 * n)

    df_train = df[:train_end].copy()
    df_val = df[train_end:val_end].copy()
    df_test = df[val_end:].copy()

    print(f"Split sizes - Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}")

    # Skip if no training data
    if len(df_train) == 0:
        print(f" Skipping {dataset_name} - no training data")
        continue

    # -----------------------------
    # STEP 1: Fit scalers on THIS CSV's training data only
    # -----------------------------
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()

    scaler_X.fit(df_train[FEATURES])
    scaler_y.fit(df_train[REG_TARGETS])

    print(f"✓ Fitted scalers on {len(df_train)} training samples from {dataset_name}")

    # Show feature ranges before/after scaling (for debugging)
    print(f"\nFeature ranges in {dataset_name}:")
    for feat in FEATURES[:3]:  # Show first 3 features as example
        orig_min, orig_max = df_train[feat].min(), df_train[feat].max()
        print(f"  {feat}: [{orig_min:.2f}, {orig_max:.2f}]")

    # -----------------------------
    # STEP 2: Transform all splits using the same scaler
    # -----------------------------
    df_train.loc[:, FEATURES] = scaler_X.transform(df_train[FEATURES])
    df_train.loc[:, REG_TARGETS] = scaler_y.transform(df_train[REG_TARGETS])

    if len(df_val) > 0:
        df_val.loc[:, FEATURES] = scaler_X.transform(df_val[FEATURES])
        df_val.loc[:, REG_TARGETS] = scaler_y.transform(df_val[REG_TARGETS])

    if len(df_test) > 0:
        df_test.loc[:, FEATURES] = scaler_X.transform(df_test[FEATURES])
        df_test.loc[:, REG_TARGETS] = scaler_y.transform(df_test[REG_TARGETS])

    # -----------------------------
    # STEP 3: Create windows
    # -----------------------------
    X_train, y_reg_train, y_cls_train = create_windows(df_train, N_STEPS)
    X_val, y_reg_val, y_cls_val = create_windows(df_val, N_STEPS)
    X_test, y_reg_test, y_cls_test = create_windows(df_test, N_STEPS)

    print(f"\nWindows created:")
    print(f"  Train: {X_train.shape[0]} windows")
    print(f"  Val:   {X_val.shape[0]} windows")
    print(f"  Test:  {X_test.shape[0]} windows")

    # -----------------------------
    # STEP 4: Create TensorDatasets
    # -----------------------------
    if X_train.size > 0:
        train_ds = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_reg_train, dtype=torch.float32),
            torch.tensor(y_cls_train, dtype=torch.float32)
        )
        train_datasets.append(train_ds)
        total_train_windows += len(train_ds)

    if X_val.size > 0:
        val_ds = TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_reg_val, dtype=torch.float32),
            torch.tensor(y_cls_val, dtype=torch.float32)
        )
        val_datasets.append(val_ds)
        total_val_windows += len(val_ds)

    if X_test.size > 0:
        test_ds = TensorDataset(
            torch.tensor(X_test, dtype=torch.float32),
            torch.tensor(y_reg_test, dtype=torch.float32),
            torch.tensor(y_cls_test, dtype=torch.float32)
        )
        test_datasets.append(test_ds)
        total_test_windows += len(test_ds)

# -----------------------------
# STEP 5: Concatenate all datasets
# -----------------------------
print("\n" + "="*80)
print("FINAL DATASET STATISTICS")
print("="*80)

if len(train_datasets) == 0:
    raise ValueError("No training data available!")
if len(val_datasets) == 0:
    raise ValueError("No validation data available!")
if len(test_datasets) == 0:
    raise ValueError("No test data available!")

train_dataset_combined = ConcatDataset(train_datasets)
val_dataset_combined = ConcatDataset(val_datasets)
test_dataset_combined = ConcatDataset(test_datasets)

print(f"Total train windows: {total_train_windows}")
print(f"Total val windows:   {total_val_windows}")
print(f"Total test windows:  {total_test_windows}")

# -----------------------------
# STEP 6: Create DataLoaders
# -----------------------------
train_loader = DataLoader(
    train_dataset_combined,
    batch_size=BATCH_SIZE,
    shuffle=True,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset_combined,
    batch_size=BATCH_SIZE,
    shuffle=False,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset_combined,
    batch_size=BATCH_SIZE*10,
    shuffle=False,
    pin_memory=True
)

print(f"\n✓ DataLoaders created successfully")
print(f"  Train batches: {len(train_loader)}")
print(f"  Val batches:   {len(val_loader)}")
print(f"  Test batches:  {len(test_loader)}")
print("="*80)
