# -*- coding: utf-8 -*-
"""Main Training Loop (Train All Models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLQHIf_EPkkJjICv4fXK3g87RdmGz0XW
"""

# -----------------------------
# Training Functions
# -----------------------------
def train_one_epoch(model, data_loader, optimizer, device, reg_criterion, cls_criterion,
                    reg_weight=1.0, cls_weight=1.0):
    model.train()
    total_loss = 0.0
    total_reg_loss = 0.0
    total_cls_loss = 0.0
    individual_reg_losses = {name: 0.0 for name in REG_TARGETS}
    count = 0

    for X_batch, y_reg_batch, y_cls_batch in data_loader:
        count += 1
        X_batch = X_batch.to(device)
        y_reg_batch = y_reg_batch.to(device)
        y_cls_batch = y_cls_batch.to(device).long().squeeze()

        optimizer.zero_grad()
        reg_pred, cls_pred = model(X_batch)

        reg_loss = reg_criterion(reg_pred, y_reg_batch)
        for i, task_name in enumerate(REG_TARGETS):
            task_loss = reg_criterion(reg_pred[:, i], y_reg_batch[:, i])
            individual_reg_losses[task_name] += task_loss.item()

        cls_loss = cls_criterion(cls_pred, y_cls_batch)
        total_batch_loss = reg_weight * reg_loss + cls_weight * cls_loss

        total_batch_loss.backward()
        optimizer.step()

        total_loss += total_batch_loss.item()
        total_reg_loss += reg_loss.item()
        total_cls_loss += cls_loss.item()

    if count == 0:
        return None

    avg_individual = {k: v / count for k, v in individual_reg_losses.items()}
    return {
        'total_loss': total_loss / count,
        'reg_loss': total_reg_loss / count,
        'cls_loss': total_cls_loss / count,
        'individual_losses': avg_individual
    }

def validate_model(model, data_loader, device, reg_criterion, cls_criterion,
                   reg_weight=1.0, cls_weight=1.0):
    model.eval()
    total_loss = 0.0
    total_reg_loss = 0.0
    total_cls_loss = 0.0
    correct = 0
    total = 0
    individual_reg_losses = {name: 0.0 for name in REG_TARGETS}
    count = 0

    with torch.no_grad():
        for X_batch, y_reg_batch, y_cls_batch in data_loader:
            count += 1
            X_batch = X_batch.to(device)
            y_reg_batch = y_reg_batch.to(device)
            y_cls_batch = y_cls_batch.to(device).long().squeeze()

            reg_pred, cls_pred = model(X_batch)

            reg_loss = reg_criterion(reg_pred, y_reg_batch)
            for i, task_name in enumerate(REG_TARGETS):
                task_loss = reg_criterion(reg_pred[:, i], y_reg_batch[:, i])
                individual_reg_losses[task_name] += task_loss.item()

            cls_loss = cls_criterion(cls_pred, y_cls_batch)
            total_batch_loss = reg_weight * reg_loss + cls_weight * cls_loss

            total_loss += total_batch_loss.item()
            total_reg_loss += reg_loss.item()
            total_cls_loss += cls_loss.item()

            _, predicted = torch.max(cls_pred, 1)
            total += y_cls_batch.size(0)
            correct += (predicted == y_cls_batch).sum().item()

    if count == 0:
        return None

    avg_individual = {k: v / count for k, v in individual_reg_losses.items()}
    accuracy = 100 * correct / total if total > 0 else 0.0

    return {
        "total_loss": total_loss / count,
        "regression_loss_total": total_reg_loss / count,
        "classification_loss": total_cls_loss / count,
        "regression_losses": avg_individual,
        "accuracy": accuracy
    }

def evaluate_and_visualize(model, test_loader, device, reg_criterion, cls_criterion, basepath, model_name="test"):
    """Evaluate model and create visualizations"""
    model.eval()

    preds_reg = []
    preds_cls = []
    real_y_reg = []
    real_y_cls = []

    with torch.no_grad():
        for X_batch, y_reg_batch, y_cls_batch in test_loader:
            X_batch = X_batch.to(device)
            reg_pred, cls_pred = model(X_batch)

            preds_reg.append(reg_pred.cpu())
            preds_cls.append(cls_pred.cpu())
            real_y_reg.append(y_reg_batch.cpu())
            real_y_cls.append(y_cls_batch.cpu())

    # Combine predictions
    preds_reg_combined = torch.cat(preds_reg, dim=0).numpy()
    real_y_reg_combined = torch.cat(real_y_reg, dim=0).numpy()
    preds_cls_combined = torch.cat(preds_cls, dim=0)
    real_y_cls_combined = torch.cat(real_y_cls, dim=0)

    # Regression plots
    for i, target_name in enumerate(REG_TARGETS):
        plt.figure(figsize=(20, 5))
        plt.plot(real_y_reg_combined[:, i], 'g:', label=f'{target_name} ground truth', linewidth=2)
        plt.plot(preds_reg_combined[:, i], 'r-', label=f'{target_name} prediction', linewidth=2)
        plt.xlabel('Sample Index')
        plt.ylabel(f'{target_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.title(f'{model_name.upper()} - {target_name} Prediction vs Ground Truth')
        fname = f"{basepath}/test_curves/{model_name}_{target_name}.png"
        plt.savefig(fname, dpi=150, bbox_inches='tight')
        plt.close()

    # Classification predictions
    if preds_cls_combined.ndim > 1 and preds_cls_combined.shape[1] > 1:
        preds_cls_labels = torch.argmax(preds_cls_combined, dim=1).numpy()
    else:
        preds_cls_labels = preds_cls_combined.numpy()

    real_y_cls_np = real_y_cls_combined.numpy()

    # Time series plot for classification
    x = np.arange(len(preds_cls_labels))
    plt.figure(figsize=(20, 5))
    plt.plot(x, real_y_cls_np, 'go-', label='True', alpha=0.7)
    plt.plot(x, preds_cls_labels, 'ro-', label='Pred', alpha=0.7)
    plt.xlabel('Sample Index')
    plt.ylabel('Class')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.title(f'{model_name.upper()} - Classification Predictions')
    plt.savefig(f"{basepath}/test_curves/{model_name}_classification.png", dpi=150, bbox_inches='tight')
    plt.close()

    # Confusion matrix
    cm_result = plot_confusion_matrix(real_y_cls_np, preds_cls_labels, model_name, basepath)

    return real_y_cls_np, preds_cls_labels

# -----------------------------
# Main Training Loop (Train All Models)
# -----------------------------
all_results = []

for model_name in MODELS_TO_TRAIN:
    print("\n" + "="*80)
    print(f"TRAINING {model_name.upper()} MODEL")
    print("="*80)

    # Reset seed for fair comparison
    set_seed()

    # Create model
    model = get_model(model_name, input_size=len(FEATURES), hidden_size=HIDDEN_SIZE,
                     dropout=DROPOUT, num_experts=NUM_EXPERTS).to(DEVICE)

    # Calculate model metrics
    model_metrics = calculate_model_metrics_thop(model, input_size=len(FEATURES),
                                                 seq_len=N_STEPS, device=DEVICE)
    print(f"\nModel Architecture: {model_name.upper()}")
    print(f"Total Parameters:     {model_metrics['total_params']:,}")
    print(f"Trainable Parameters: {model_metrics['trainable_params']:,}")
    print(f"Model Size (MB):      {model_metrics['model_size_mb']:.2f}")
    print(f"FLOPs:                {model_metrics['flops_readable']}")

    # Setup training
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)
    reg_criterion = nn.MSELoss()
    cls_criterion = nn.CrossEntropyLoss()

    best_epoch = 0
    best_loss = 1e12
    best_state = None
    metrics_history = {
        'train_loss': [],
        'val_total_loss': [],
        'val_reg_loss': [],
        'val_cls_loss': [],
        'val_rsrp': [],
        'val_rsrq': [],
        'val_sinr': [],
        'val_acc': []
    }

    print("\n" + "-"*80)
    print("STARTING TRAINING")
    print("-"*80)

    train_start = time.perf_counter()

    for epoch in range(EPOCHS):
        # Training
        train_res = train_one_epoch(model, train_loader, optimizer, DEVICE,
                                    reg_criterion, cls_criterion)

        if train_res is None:
            print("No training data; stopping.")
            break

        metrics_history['train_loss'].append(train_res['total_loss'])

        # Validation
        val_res = validate_model(model, val_loader, DEVICE, reg_criterion, cls_criterion)

        if val_res is None:
            print("No validation data; stopping.")
            break

        metrics_history['val_total_loss'].append(val_res['total_loss'])
        metrics_history['val_reg_loss'].append(val_res['regression_loss_total'])
        metrics_history['val_cls_loss'].append(val_res['classification_loss'])
        metrics_history['val_rsrp'].append(val_res['regression_losses']['RSRP'])
        metrics_history['val_rsrq'].append(val_res['regression_losses']['RSRQ'])
        metrics_history['val_sinr'].append(val_res['regression_losses']['SINR'])
        metrics_history['val_acc'].append(val_res['accuracy'])

        # Save best model
        if val_res['total_loss'] < best_loss:
            best_loss = val_res['total_loss']
            best_epoch = epoch
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            torch.save(best_state, os.path.join(BASEPATH, f"best_{model_name}.pt"))

        # Print progress
        if epoch % max(1, EPOCHS//10) == 0 or epoch < 5:
            print(f"Epoch {epoch+1}/{EPOCHS} | "
                  f"Train: {train_res['total_loss']:.6f} | "
                  f"Val: {val_res['total_loss']:.6f} | "
                  f"Acc: {val_res['accuracy']:.2f}% | "
                  f"Best: {best_epoch}")

    train_end = time.perf_counter()
    total_train_time = train_end - train_start

    print("\n" + "-"*80)
    print(f"TRAINING COMPLETED FOR {model_name.upper()}")
    print("-"*80)
    print(f"Total Training Time: {total_train_time:.2f}s")
    print(f"Best Epoch: {best_epoch}")
    print(f"Best Val Loss: {best_loss:.6f}")

    # -----------------------------
    # Load Best Model and Evaluate on Test Set
    # -----------------------------
    print(f"\nEvaluating {model_name.upper()} on test set...")

    model.load_state_dict(best_state)

    # Calculate inference time
    test_start = time.perf_counter()
    avg_inf_time = calculate_inference_time(model, test_loader, DEVICE, num_runs=100)
    test_end = time.perf_counter()
    total_test_time = test_end - test_start

    # Evaluate on test set
    test_res = validate_model(model, test_loader, DEVICE, reg_criterion, cls_criterion)

    # Create visualizations
    print(f"Generating visualizations for {model_name.upper()}...")
    evaluate_and_visualize(model, test_loader, DEVICE, reg_criterion, cls_criterion,
                          BASEPATH, model_name)

    # Plot loss curves for this model
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    plt.plot(metrics_history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    plt.plot(metrics_history['val_total_loss'], 'r-', label='Val Total Loss', linewidth=2)
    plt.axvline(x=best_epoch, color='orange', linestyle='--', linewidth=1, label=f'Best ({best_epoch})')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{model_name.upper()} - Total Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 2, 2)
    plt.plot(metrics_history['val_rsrp'], 'r-', label='RSRP', linewidth=1.5)
    plt.plot(metrics_history['val_rsrq'], 'g-', label='RSRQ', linewidth=1.5)
    plt.plot(metrics_history['val_sinr'], 'b-', label='SINR', linewidth=1.5)
    plt.axvline(x=best_epoch, color='orange', linestyle='--', linewidth=1)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{model_name.upper()} - Regression Losses')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 2, 3)
    plt.plot(metrics_history['val_acc'], 'purple', linewidth=2)
    plt.axvline(x=best_epoch, color='orange', linestyle='--', linewidth=1)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title(f'{model_name.upper()} - Classification Accuracy')
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 2, 4)
    plt.plot(metrics_history['val_reg_loss'], 'b-', label='Regression', linewidth=2)
    plt.plot(metrics_history['val_cls_loss'], 'r-', label='Classification', linewidth=2)
    plt.axvline(x=best_epoch, color='orange', linestyle='--', linewidth=1)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{model_name.upper()} - Reg vs Cls Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{BASEPATH}/loss_curves/{model_name}_loss_curves.png", dpi=150, bbox_inches='tight')
    plt.close()

    # Store results
    results_dict = {
        'model': model_name,
        'total_test_loss': test_res['total_loss'],
        'regression_loss_total': test_res['regression_loss_total'],
        'classification_loss': test_res['classification_loss'],
        'rsrp_loss': test_res['regression_losses']['RSRP'],
        'rsrq_loss': test_res['regression_losses']['RSRQ'],
        'sinr_loss': test_res['regression_losses']['SINR'],
        'accuracy': test_res['accuracy'],
        'best_epoch': best_epoch,
        'best_val_loss': best_loss,
        'training_time_s': total_train_time,
        'test_time_s': total_test_time,
        'avg_inference_ms': (avg_inf_time*1000) if avg_inf_time else None,
        'total_params': model_metrics['total_params'],
        'trainable_params': model_metrics['trainable_params'],
        'model_size_mb': model_metrics['model_size_mb'],
        'flops': model_metrics['flops_readable']
    }
    all_results.append(results_dict)

    # Print results for this model
    print("\n" + "="*80)
    print(f"FINAL TEST RESULTS - {model_name.upper()}")
    print("="*80)
    print(f"Total Test Loss:          {test_res['total_loss']:.6f}")
    print(f"Regression Loss Total:    {test_res['regression_loss_total']:.6f}")
    print(f"  - RSRP Loss:            {test_res['regression_losses']['RSRP']:.6f}")
    print(f"  - RSRQ Loss:            {test_res['regression_losses']['RSRQ']:.6f}")
    print(f"  - SINR Loss:            {test_res['regression_losses']['SINR']:.6f}")
    print(f"Classification Loss:      {test_res['classification_loss']:.6f}")
    print(f"Classification Accuracy:  {test_res['accuracy']:.2f}%")
    print(f"Training Time:            {total_train_time:.2f}s")
    print(f"Avg Inference Time:       {(avg_inf_time*1000):.2f}ms" if avg_inf_time else "N/A")
    print("="*80)

# -----------------------------
# Save All Results and Create Comparison
# -----------------------------
print("\n" + "="*80)
print("SAVING RESULTS AND CREATING COMPARISONS")
print("="*80)

# Save all results to CSV
all_results_df = pd.DataFrame(all_results)
all_results_df.to_csv(f"{BASEPATH}/all_models_results.csv", index=False)
print("âœ“ All results saved to CSV")

# Print comparison table
print("\n" + "="*80)
print("MODEL COMPARISON")
print("="*80)
comparison_cols = ['model', 'accuracy', 'total_test_loss', 'regression_loss_total',
                   'classification_loss', 'training_time_s', 'avg_inference_ms', 'total_params']
print(all_results_df[comparison_cols].to_string(index=False))
print("="*80)

# Find best model
best_model_idx = all_results_df['total_test_loss'].idxmin()
best_model_name = all_results_df.loc[best_model_idx, 'model']
print(f"\nðŸ† BEST MODEL: {best_model_name.upper()}")
print(f"   Test Loss: {all_results_df.loc[best_model_idx, 'total_test_loss']:.6f}")
print(f"   Accuracy:  {all_results_df.loc[best_model_idx, 'accuracy']:.2f}%")

# Create comparison plots
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Plot 1: Test Loss Comparison
axes[0, 0].bar(all_results_df['model'], all_results_df['total_test_loss'])
axes[0, 0].set_title('Total Test Loss')
axes[0, 0].set_xlabel('Model')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Accuracy Comparison
axes[0, 1].bar(all_results_df['model'], all_results_df['accuracy'], color='green')
axes[0, 1].set_title('Classification Accuracy')
axes[0, 1].set_xlabel('Model')
axes[0, 1].set_ylabel('Accuracy (%)')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Regression Loss Comparison
axes[0, 2].bar(all_results_df['model'], all_results_df['regression_loss_total'], color='orange')
axes[0, 2].set_title('Regression Loss')
axes[0, 2].set_xlabel('Model')
axes[0, 2].set_ylabel('Loss')
axes[0, 2].tick_params(axis='x', rotation=45)
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Training Time Comparison
axes[1, 0].bar(all_results_df['model'], all_results_df['training_time_s'], color='purple')
axes[1, 0].set_title('Training Time')
axes[1, 0].set_xlabel('Model')
axes[1, 0].set_ylabel('Time (s)')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(True, alpha=0.3)

# Plot 5: Inference Time Comparison
axes[1, 1].bar(all_results_df['model'], all_results_df['avg_inference_ms'], color='red')
axes[1, 1].set_title('Inference Time')
axes[1, 1].set_xlabel('Model')
axes[1, 1].set_ylabel('Time (ms)')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, alpha=0.3)

# Plot 6: Model Size Comparison
axes[1, 2].bar(all_results_df['model'], all_results_df['total_params']/1e6, color='brown')
axes[1, 2].set_title('Model Size')
axes[1, 2].set_xlabel('Model')
axes[1, 2].set_ylabel('Parameters (Millions)')
axes[1, 2].tick_params(axis='x', rotation=45)
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{BASEPATH}/model_comparison.png", dpi=150, bbox_inches='tight')
plt.close()
print("âœ“ Comparison plots saved")

print("\n" + "="*80)
print("âœ“ ALL EXPERIMENTS COMPLETED!")
print("="*80)
print(f"Results saved to:           {BASEPATH}/final_results.csv")
print(f"Confusion matrices saved to: {BASEPATH}/confusion_matrices/")
print(f"Loss curves saved to:        {BASEPATH}/loss_curves/")
print(f"Test visualizations saved to: {BASEPATH}/test_curves/")
print("="*80)
